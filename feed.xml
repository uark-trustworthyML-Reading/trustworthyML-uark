<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://trustworthyML-uark.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://trustworthyML-uark.github.io/" rel="alternate" type="text/html" /><updated>2024-03-22T17:31:29-05:00</updated><id>https://trustworthyML-uark.github.io/feed.xml</id><title type="html">Trustworthy ML Reading Group</title><subtitle>Here lies the Trustworthy ML reading group at UARK. Check us out!</subtitle><entry><title type="html">Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification</title><link href="https://trustworthyML-uark.github.io/2024/03/13/mtg8.html" rel="alternate" type="text/html" title="Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification" /><published>2024-03-13T00:00:00-05:00</published><updated>2024-03-13T00:00:00-05:00</updated><id>https://trustworthyML-uark.github.io/2024/03/13/mtg8</id><content type="html" xml:base="https://trustworthyML-uark.github.io/2024/03/13/mtg8.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2301.11562v7.pdf">Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification</a> - <em>A. Feder Cooper, K. Lee, M. Zahrah Choksi, S. Barocas, C. De Sa, J. Grimmelmann, J. Kleinberg, S. Sen, B. Zhang</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/13Mar_ArbitrarinessAndSocialPrediction-PaperClub.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="fairness" /><summary type="html"><![CDATA[Check out the paper Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification - A. Feder Cooper, K. Lee, M. Zahrah Choksi, S. Barocas, C. De Sa, J. Grimmelmann, J. Kleinberg, S. Sen, B. Zhang]]></summary></entry><entry><title type="html">Runaround + Code 8</title><link href="https://trustworthyML-uark.github.io/2024/03/05/mtg7.html" rel="alternate" type="text/html" title="Runaround + Code 8" /><published>2024-03-05T23:00:00-06:00</published><updated>2024-03-05T23:00:00-06:00</updated><id>https://trustworthyML-uark.github.io/2024/03/05/mtg7</id><content type="html" xml:base="https://trustworthyML-uark.github.io/2024/03/05/mtg7.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf">Differentially Private Fair Learning</a> - <em>Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/6Mar_Why AI.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="philosophy" /><category term="non-technical" /><summary type="html"><![CDATA[Check out the paper Differentially Private Fair Learning - Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman]]></summary></entry><entry><title type="html">Runaround - Isaac Asimov</title><link href="https://trustworthyML-uark.github.io/2024/02/27/mtg6.html" rel="alternate" type="text/html" title="Runaround - Isaac Asimov" /><published>2024-02-27T23:00:00-06:00</published><updated>2024-02-27T23:00:00-06:00</updated><id>https://trustworthyML-uark.github.io/2024/02/27/mtg6</id><content type="html" xml:base="https://trustworthyML-uark.github.io/2024/02/27/mtg6.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf">Differentially Private Fair Learning</a> - <em>Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/28Feb_Runaround.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="philosophy" /><category term="non-technical" /><summary type="html"><![CDATA[Check out the paper Differentially Private Fair Learning - Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman]]></summary></entry><entry><title type="html">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</title><link href="https://trustworthyML-uark.github.io/2024/02/20/mtg5.html" rel="alternate" type="text/html" title="Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training" /><published>2024-02-20T23:00:00-06:00</published><updated>2024-02-20T23:00:00-06:00</updated><id>https://trustworthyML-uark.github.io/2024/02/20/mtg5</id><content type="html" xml:base="https://trustworthyML-uark.github.io/2024/02/20/mtg5.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf">Differentially Private Fair Learning</a> - <em>Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/21Feb_Sleeper Agents.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="LLMs" /><category term="safety" /><category term="adversarial" /><summary type="html"><![CDATA[Check out the paper Differentially Private Fair Learning - Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman]]></summary></entry><entry><title type="html">Model Explanations with Differential Privacy</title><link href="https://trustworthyML-uark.github.io/2024/02/13/mtg4.html" rel="alternate" type="text/html" title="Model Explanations with Differential Privacy" /><published>2024-02-13T23:00:00-06:00</published><updated>2024-02-13T23:00:00-06:00</updated><id>https://trustworthyML-uark.github.io/2024/02/13/mtg4</id><content type="html" xml:base="https://trustworthyML-uark.github.io/2024/02/13/mtg4.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://dl.acm.org/doi/pdf/10.1145/3531146.3533235">Model Explanations with Differential Privacy</a> - <em>N. Patel, R. Shokri, and Y. Zick. 2022</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/14Feb_Model_Explantions_with_DP.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="interpretability" /><category term="privacy" /><summary type="html"><![CDATA[Check out the paper Model Explanations with Differential Privacy - N. Patel, R. Shokri, and Y. Zick. 2022]]></summary></entry><entry><title type="html">Causal parrots: Large language models may talk causality but are not causal</title><link href="https://trustworthyML-uark.github.io/2024/02/06/mtg3.html" rel="alternate" type="text/html" title="Causal parrots: Large language models may talk causality but are not causal" /><published>2024-02-06T23:00:00-06:00</published><updated>2024-02-06T23:00:00-06:00</updated><id>https://trustworthyML-uark.github.io/2024/02/06/mtg3</id><content type="html" xml:base="https://trustworthyML-uark.github.io/2024/02/06/mtg3.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://openreview.net/forum?id=tv46tCzs83">Causal parrots: Large language models may talk causality but are not causal</a> - <em>Zečević, M., Willig, M., Dhami, D. S., &amp; Kersting, K. (2023)</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/7Feb_causalparrots.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="LLMs" /><category term="Causality" /><summary type="html"><![CDATA[Check out the paper Causal parrots: Large language models may talk causality but are not causal - Zečević, M., Willig, M., Dhami, D. S., &amp; Kersting, K. (2023)]]></summary></entry><entry><title type="html">Differentially Private Fair Learning</title><link href="https://trustworthyML-uark.github.io/2024/01/30/mtg2.html" rel="alternate" type="text/html" title="Differentially Private Fair Learning" /><published>2024-01-30T23:00:00-06:00</published><updated>2024-01-30T23:00:00-06:00</updated><id>https://trustworthyML-uark.github.io/2024/01/30/mtg2</id><content type="html" xml:base="https://trustworthyML-uark.github.io/2024/01/30/mtg2.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf">Differentially Private Fair Learning</a> - <em>Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/31Jan_DP_Fairness_paper.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="privacy" /><category term="fairness" /><summary type="html"><![CDATA[Check out the paper Differentially Private Fair Learning - Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman]]></summary></entry><entry><title type="html">Welcome / Intro</title><link href="https://trustworthyML-uark.github.io/2024/01/23/mtg1.html" rel="alternate" type="text/html" title="Welcome / Intro" /><published>2024-01-23T23:00:00-06:00</published><updated>2024-01-23T23:00:00-06:00</updated><id>https://trustworthyML-uark.github.io/2024/01/23/mtg1</id><content type="html" xml:base="https://trustworthyML-uark.github.io/2024/01/23/mtg1.html"><![CDATA[<p>This week we covered basic expectations and topics we hope to cover. Nothing big here!</p>]]></content><author><name></name></author><category term="non-technical" /><summary type="html"><![CDATA[This week we covered basic expectations and topics we hope to cover. Nothing big here!]]></summary></entry></feed>
