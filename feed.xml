<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://trustworthyml-uark.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://trustworthyml-uark.github.io/" rel="alternate" type="text/html" /><updated>2024-10-07T10:45:57-05:00</updated><id>https://trustworthyml-uark.github.io/feed.xml</id><title type="html">Trustworthy ML Reading Group</title><subtitle>Here lies the Trustworthy ML reading group at UARK. Check us out!</subtitle><entry><title type="html">Towards A Rigorous Science of Interpretable Machine Learning</title><link href="https://trustworthyml-uark.github.io/2024/10/01/mtg16.html" rel="alternate" type="text/html" title="Towards A Rigorous Science of Interpretable Machine Learning" /><published>2024-10-01T00:00:00-05:00</published><updated>2024-10-01T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/10/01/mtg16</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/10/01/mtg16.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1702.08608">Towards A Rigorous Science of Interpretable Machine Learning</a> <em>Finale Doshi-Velez and Been Kim</em></li>
</ul>

<h3 id="abstract">Abstract</h3>
<p>As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.</p>

<h3 id="check-out-the-slides">Check out the slides</h3>
<!---->
<p>PDF<a href="/assets/slides/1Oct_RigourousScience.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="non-technical" /><category term="interpretability" /><summary type="html"><![CDATA[Check out the paper Towards A Rigorous Science of Interpretable Machine Learning Finale Doshi-Velez and Been Kim]]></summary></entry><entry><title type="html">Machine Unlearning</title><link href="https://trustworthyml-uark.github.io/2024/09/24/mtg15.html" rel="alternate" type="text/html" title="Machine Unlearning" /><published>2024-09-24T00:00:00-05:00</published><updated>2024-09-24T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/09/24/mtg15</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/09/24/mtg15.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1912.03817">Machine Unlearning</a></li>
</ul>

<h3 id="abstract">Abstract</h3>
<p>Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult. We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning. Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63x, and 2.45x for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36x in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.</p>

<h3 id="check-out-the-slides">Check out the slides</h3>
<!---->
<p>PDF<a href="/assets/slides/24Sep_MachineUnlearning.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="privacy" /><summary type="html"><![CDATA[Check out the paper Machine Unlearning]]></summary></entry><entry><title type="html">AI Art and its Impact on Artists</title><link href="https://trustworthyml-uark.github.io/2024/06/26/mtg14.html" rel="alternate" type="text/html" title="AI Art and its Impact on Artists" /><published>2024-06-26T00:00:00-05:00</published><updated>2024-06-26T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/06/26/mtg14</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/06/26/mtg14.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://dl.acm.org/doi/10.1145/3600211.3604681">AI Art and its Impact on Artists</a></li>
</ul>

<h3 id="abstract">Abstract</h3>
<p>The last 3 years have resulted in machine learning (ML)-based image generators with the ability to output consistently higher quality images based on natural language prompts as inputs. As a result, many popular commercial “generative AI Art” products have entered the market, making generative AI an estimated $48B industry [125]. However, many professional artists have spoken up about the harms they have experienced due to the proliferation of large scale image generators trained on image/text pairs from the Internet. In this paper, we review some of these harms which include reputational damage, economic loss, plagiarism and copyright infringement. To guard against these issues while reaping the potential benefits of image generators, we provide recommendations such as regulation that forces organizations to disclose their training data, and tools that help artists prevent using their content as training data without their consent.</p>

<!--### Check out the slides-->
<!---->
<!--<p>PDF<a href="/assets/slides/24April_inherentPrivacy.pdf"> here</a>.</p>-->]]></content><author><name></name></author><category term="ethics" /><category term="fairness" /><category term="privacy" /><summary type="html"><![CDATA[Check out the paper AI Art and its Impact on Artists]]></summary></entry><entry><title type="html">Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions</title><link href="https://trustworthyml-uark.github.io/2024/05/01/mtg13.html" rel="alternate" type="text/html" title="Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions" /><published>2024-05-01T00:00:00-05:00</published><updated>2024-05-01T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/05/01/mtg13</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/05/01/mtg13.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2309.08678">Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions</a></li>
</ul>

<h3 id="abstract">Abstract</h3>
<p>How to properly set the privacy parameter in
differential privacy (DP) has been an open question in DP
research since it was first proposed in 2006. In this work, we
demonstrate the ability of influence functions to offer insight
into how a specific privacy parameter value will affect a model’s
test loss in the randomized response-based local DP setting. Our
proposed method allows a data curator to select the privacy
parameter best aligned with their allowed privacy-utility trade-off
without requiring heavy computation such as extensive model
retraining and data privatization. We consider multiple common
randomization scenarios, such as performing randomized response
over the features, and/or over the labels, as well as the more
complex case of applying a class-dependent label noise correction
method to offset the noise incurred by randomization. Further, we
provide a detailed discussion over the computational complexity
of our proposed approach inclusive of an empirical analysis.
Through empirical evaluations we show that for both binary and
multi-class settings, influence functions are able to approximate
the true change in test loss that occurs when randomized response
is applied over features and/or labels with small mean absolute
error, especially in cases where noise correction methods are
applied.</p>

<!--### Check out the slides-->
<!---->
<!--<p>PDF<a href="/assets/slides/24April_inherentPrivacy.pdf"> here</a>.</p>-->]]></content><author><name></name></author><category term="privacy" /><summary type="html"><![CDATA[Check out the paper Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions]]></summary></entry><entry><title type="html">Probabilistic Dataset Reconstruction from Interpretable Models</title><link href="https://trustworthyml-uark.github.io/2024/04/24/mtg12.html" rel="alternate" type="text/html" title="Probabilistic Dataset Reconstruction from Interpretable Models" /><published>2024-04-24T00:00:00-05:00</published><updated>2024-04-24T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/04/24/mtg12</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/04/24/mtg12.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2308.15099.pdf">Probabilistic Dataset Reconstruction from Interpretable Models</a></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/24April_inherentPrivacy.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="interpretability" /><category term="privacy" /><summary type="html"><![CDATA[Check out the paper Probabilistic Dataset Reconstruction from Interpretable Models]]></summary></entry><entry><title type="html">Risks From Learned Optimization in Advanced Machine Learning Systems</title><link href="https://trustworthyml-uark.github.io/2024/04/17/mtg11.html" rel="alternate" type="text/html" title="Risks From Learned Optimization in Advanced Machine Learning Systems" /><published>2024-04-17T00:00:00-05:00</published><updated>2024-04-17T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/04/17/mtg11</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/04/17/mtg11.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1906.01820.pdf">Risks From Learned Optimization in Advanced Machine Learning Systems</a></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>
<!---->
<!--<p>PDF<a href="/assets/slides/27Mar_Underspecification Presentation.pdf"> here</a>.</p>-->]]></content><author><name></name></author><category term="adversarial" /><category term="safety" /><summary type="html"><![CDATA[Check out the paper Risks From Learned Optimization in Advanced Machine Learning Systems]]></summary></entry><entry><title type="html">Causal Fairness Field Guide: Perspectives from Social and Formal Sciences</title><link href="https://trustworthyml-uark.github.io/2024/04/03/mtg10.html" rel="alternate" type="text/html" title="Causal Fairness Field Guide: Perspectives from Social and Formal Sciences" /><published>2024-04-03T00:00:00-05:00</published><updated>2024-04-03T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/04/03/mtg10</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/04/03/mtg10.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://www.frontiersin.org/articles/10.3389/fdata.2022.892837/full">Causal Fairness Field Guide: Perspectives from Social and Formal Sciences</a></li>
  <li><a href="https://fairmlbook.org/pdf/fairmlbook.pdf#causality">Causality Chapter from the Fair ML Book</a></li>
</ul>

<!--### Check out the slides-->
<!---->
<!--<p>PDF<a href="/assets/slides/27Mar_Underspecification Presentation.pdf"> here</a>.</p>-->]]></content><author><name></name></author><category term="fairness" /><category term="causality" /><category term="ethics" /><summary type="html"><![CDATA[Check out the paper Causal Fairness Field Guide: Perspectives from Social and Formal Sciences Causality Chapter from the Fair ML Book]]></summary></entry><entry><title type="html">Underspecification Presents Challenges for Credibility in Modern Machine Learning</title><link href="https://trustworthyml-uark.github.io/2024/03/27/mtg9.html" rel="alternate" type="text/html" title="Underspecification Presents Challenges for Credibility in Modern Machine Learning" /><published>2024-03-27T00:00:00-05:00</published><updated>2024-03-27T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/03/27/mtg9</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/03/27/mtg9.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://www.technologyreview.com/2020/11/18/1012234/training-machine-learning-broken-real-world-heath-nlp-computer-vision/">MIT Technology Review Article</a></li>
  <li><a href="https://arxiv.org/pdf/2011.03395.pdf">Underspecification Presents Challenges for Credibility in
Modern Machine Learning</a></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/27Mar_Underspecification Presentation.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="underspecification" /><category term="philosophy" /><category term="fairness" /><category term="NLP" /><category term="computer_vision" /><summary type="html"><![CDATA[Check out the paper MIT Technology Review Article Underspecification Presents Challenges for Credibility in Modern Machine Learning]]></summary></entry><entry><title type="html">Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification</title><link href="https://trustworthyml-uark.github.io/2024/03/13/mtg8.html" rel="alternate" type="text/html" title="Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification" /><published>2024-03-13T00:00:00-05:00</published><updated>2024-03-13T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/03/13/mtg8</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/03/13/mtg8.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2301.11562v7.pdf">Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification</a> - <em>A. Feder Cooper, K. Lee, M. Zahrah Choksi, S. Barocas, C. De Sa, J. Grimmelmann, J. Kleinberg, S. Sen, B. Zhang</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/13Mar_ArbitrarinessAndSocialPrediction-PaperClub.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="fairness" /><summary type="html"><![CDATA[Check out the paper Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification - A. Feder Cooper, K. Lee, M. Zahrah Choksi, S. Barocas, C. De Sa, J. Grimmelmann, J. Kleinberg, S. Sen, B. Zhang]]></summary></entry><entry><title type="html">Runaround + Code 8</title><link href="https://trustworthyml-uark.github.io/2024/03/05/mtg7.html" rel="alternate" type="text/html" title="Runaround + Code 8" /><published>2024-03-05T23:00:00-06:00</published><updated>2024-03-05T23:00:00-06:00</updated><id>https://trustworthyml-uark.github.io/2024/03/05/mtg7</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/03/05/mtg7.html"><![CDATA[<h3 id="check-out-the-story-and-short">Check out the Story and Short</h3>
<ul>
  <li><a href="https://tcw.org/lefty/Short%20Stories/Runaround.pdf">Runaround - Isaac Asimov</a></li>
  <li><a href="https://www.youtube.com/watch?v=DqO90q0WZ0M">Code 8 (Youtube)</a> <em>Short Film</em></li>
</ul>

<p><em>note:</em> Code 8 was turned into a full length feature (now on Netflix).</p>]]></content><author><name></name></author><category term="philosophy" /><category term="non-technical" /><summary type="html"><![CDATA[Check out the Story and Short Runaround - Isaac Asimov Code 8 (Youtube) Short Film]]></summary></entry></feed>