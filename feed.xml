<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://trustworthyml-uark.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://trustworthyml-uark.github.io/" rel="alternate" type="text/html" /><updated>2024-08-22T14:58:34-05:00</updated><id>https://trustworthyml-uark.github.io/feed.xml</id><title type="html">Trustworthy ML Reading Group</title><subtitle>Here lies the Trustworthy ML reading group at UARK. Check us out!</subtitle><entry><title type="html">Probabilistic Dataset Reconstruction from Interpretable Models</title><link href="https://trustworthyml-uark.github.io/2024/05/01/mtg13.html" rel="alternate" type="text/html" title="Probabilistic Dataset Reconstruction from Interpretable Models" /><published>2024-05-01T00:00:00-05:00</published><updated>2024-05-01T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/05/01/mtg13</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/05/01/mtg13.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2309.08678">Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions</a></li>
</ul>

<h3 id="abstract">Abstract</h3>
<p>How to properly set the privacy parameter in
differential privacy (DP) has been an open question in DP
research since it was first proposed in 2006. In this work, we
demonstrate the ability of influence functions to offer insight
into how a specific privacy parameter value will affect a modelâ€™s
test loss in the randomized response-based local DP setting. Our
proposed method allows a data curator to select the privacy
parameter best aligned with their allowed privacy-utility trade-off
without requiring heavy computation such as extensive model
retraining and data privatization. We consider multiple common
randomization scenarios, such as performing randomized response
over the features, and/or over the labels, as well as the more
complex case of applying a class-dependent label noise correction
method to offset the noise incurred by randomization. Further, we
provide a detailed discussion over the computational complexity
of our proposed approach inclusive of an empirical analysis.
Through empirical evaluations we show that for both binary and
multi-class settings, influence functions are able to approximate
the true change in test loss that occurs when randomized response
is applied over features and/or labels with small mean absolute
error, especially in cases where noise correction methods are
applied.</p>

<!--### Check out the slides-->
<!---->
<!--<p>PDF<a href="/assets/slides/24April_inherentPrivacy.pdf"> here</a>.</p>-->]]></content><author><name></name></author><category term="privacy" /><summary type="html"><![CDATA[Check out the paper Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions]]></summary></entry><entry><title type="html">Probabilistic Dataset Reconstruction from Interpretable Models</title><link href="https://trustworthyml-uark.github.io/2024/04/24/mtg12.html" rel="alternate" type="text/html" title="Probabilistic Dataset Reconstruction from Interpretable Models" /><published>2024-04-24T00:00:00-05:00</published><updated>2024-04-24T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/04/24/mtg12</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/04/24/mtg12.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2308.15099.pdf">Probabilistic Dataset Reconstruction from Interpretable Models</a></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/24April_inherentPrivacy.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="interpretability" /><category term="privacy" /><summary type="html"><![CDATA[Check out the paper Probabilistic Dataset Reconstruction from Interpretable Models]]></summary></entry><entry><title type="html">Risks From Learned Optimization in Advanced Machine Learning Systems</title><link href="https://trustworthyml-uark.github.io/2024/04/17/mtg11.html" rel="alternate" type="text/html" title="Risks From Learned Optimization in Advanced Machine Learning Systems" /><published>2024-04-17T00:00:00-05:00</published><updated>2024-04-17T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/04/17/mtg11</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/04/17/mtg11.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1906.01820.pdf">Risks From Learned Optimization in Advanced Machine Learning Systems</a></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>
<!---->
<!--<p>PDF<a href="/assets/slides/27Mar_Underspecification Presentation.pdf"> here</a>.</p>-->]]></content><author><name></name></author><category term="adversarial" /><category term="safety" /><summary type="html"><![CDATA[Check out the paper Risks From Learned Optimization in Advanced Machine Learning Systems]]></summary></entry><entry><title type="html">Causal Fairness Field Guide: Perspectives from Social and Formal Sciences</title><link href="https://trustworthyml-uark.github.io/2024/04/03/mtg10.html" rel="alternate" type="text/html" title="Causal Fairness Field Guide: Perspectives from Social and Formal Sciences" /><published>2024-04-03T00:00:00-05:00</published><updated>2024-04-03T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/04/03/mtg10</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/04/03/mtg10.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://www.frontiersin.org/articles/10.3389/fdata.2022.892837/full">Causal Fairness Field Guide: Perspectives from Social and Formal Sciences</a></li>
  <li><a href="https://fairmlbook.org/pdf/fairmlbook.pdf#causality">Causality Chapter from the Fair ML Book</a></li>
</ul>

<!--### Check out the slides-->
<!---->
<!--<p>PDF<a href="/assets/slides/27Mar_Underspecification Presentation.pdf"> here</a>.</p>-->]]></content><author><name></name></author><category term="fairness" /><category term="causality" /><category term="ethics" /><summary type="html"><![CDATA[Check out the paper Causal Fairness Field Guide: Perspectives from Social and Formal Sciences Causality Chapter from the Fair ML Book]]></summary></entry><entry><title type="html">Underspecification Presents Challenges for Credibility in Modern Machine Learning</title><link href="https://trustworthyml-uark.github.io/2024/03/27/mtg9.html" rel="alternate" type="text/html" title="Underspecification Presents Challenges for Credibility in Modern Machine Learning" /><published>2024-03-27T00:00:00-05:00</published><updated>2024-03-27T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/03/27/mtg9</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/03/27/mtg9.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://www.technologyreview.com/2020/11/18/1012234/training-machine-learning-broken-real-world-heath-nlp-computer-vision/">MIT Technology Review Article</a></li>
  <li><a href="https://arxiv.org/pdf/2011.03395.pdf">Underspecification Presents Challenges for Credibility in
Modern Machine Learning</a></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/27Mar_Underspecification Presentation.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="underspecification" /><category term="philosophy" /><category term="fairness" /><category term="NLP" /><category term="computer_vision" /><summary type="html"><![CDATA[Check out the paper MIT Technology Review Article Underspecification Presents Challenges for Credibility in Modern Machine Learning]]></summary></entry><entry><title type="html">Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification</title><link href="https://trustworthyml-uark.github.io/2024/03/13/mtg8.html" rel="alternate" type="text/html" title="Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification" /><published>2024-03-13T00:00:00-05:00</published><updated>2024-03-13T00:00:00-05:00</updated><id>https://trustworthyml-uark.github.io/2024/03/13/mtg8</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/03/13/mtg8.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2301.11562v7.pdf">Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification</a> - <em>A. Feder Cooper, K. Lee, M. Zahrah Choksi, S. Barocas, C. De Sa, J. Grimmelmann, J. Kleinberg, S. Sen, B. Zhang</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/13Mar_ArbitrarinessAndSocialPrediction-PaperClub.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="fairness" /><summary type="html"><![CDATA[Check out the paper Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification - A. Feder Cooper, K. Lee, M. Zahrah Choksi, S. Barocas, C. De Sa, J. Grimmelmann, J. Kleinberg, S. Sen, B. Zhang]]></summary></entry><entry><title type="html">Runaround + Code 8</title><link href="https://trustworthyml-uark.github.io/2024/03/05/mtg7.html" rel="alternate" type="text/html" title="Runaround + Code 8" /><published>2024-03-05T23:00:00-06:00</published><updated>2024-03-05T23:00:00-06:00</updated><id>https://trustworthyml-uark.github.io/2024/03/05/mtg7</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/03/05/mtg7.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf">Differentially Private Fair Learning</a> - <em>Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/6Mar_Why AI.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="philosophy" /><category term="non-technical" /><summary type="html"><![CDATA[Check out the paper Differentially Private Fair Learning - Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman]]></summary></entry><entry><title type="html">Runaround - Isaac Asimov</title><link href="https://trustworthyml-uark.github.io/2024/02/27/mtg6.html" rel="alternate" type="text/html" title="Runaround - Isaac Asimov" /><published>2024-02-27T23:00:00-06:00</published><updated>2024-02-27T23:00:00-06:00</updated><id>https://trustworthyml-uark.github.io/2024/02/27/mtg6</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/02/27/mtg6.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf">Differentially Private Fair Learning</a> - <em>Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/28Feb_Runaround.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="philosophy" /><category term="non-technical" /><summary type="html"><![CDATA[Check out the paper Differentially Private Fair Learning - Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman]]></summary></entry><entry><title type="html">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</title><link href="https://trustworthyml-uark.github.io/2024/02/20/mtg5.html" rel="alternate" type="text/html" title="Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training" /><published>2024-02-20T23:00:00-06:00</published><updated>2024-02-20T23:00:00-06:00</updated><id>https://trustworthyml-uark.github.io/2024/02/20/mtg5</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/02/20/mtg5.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf">Differentially Private Fair Learning</a> - <em>Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/21Feb_Sleeper Agents.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="LLMs" /><category term="safety" /><category term="adversarial" /><category term="NLP" /><summary type="html"><![CDATA[Check out the paper Differentially Private Fair Learning - Matthew Jagielski, Michael Kearns, Jieming Mao, Aaron Oprea, Alina Roth, Saeed Sharifi-Malvajerdi, and Jonathan Ullman]]></summary></entry><entry><title type="html">Model Explanations with Differential Privacy</title><link href="https://trustworthyml-uark.github.io/2024/02/13/mtg4.html" rel="alternate" type="text/html" title="Model Explanations with Differential Privacy" /><published>2024-02-13T23:00:00-06:00</published><updated>2024-02-13T23:00:00-06:00</updated><id>https://trustworthyml-uark.github.io/2024/02/13/mtg4</id><content type="html" xml:base="https://trustworthyml-uark.github.io/2024/02/13/mtg4.html"><![CDATA[<h3 id="check-out-the-paper">Check out the paper</h3>
<ul>
  <li><a href="https://dl.acm.org/doi/pdf/10.1145/3531146.3533235">Model Explanations with Differential Privacy</a> - <em>N. Patel, R. Shokri, and Y. Zick. 2022</em></li>
</ul>

<h3 id="check-out-the-slides">Check out the slides</h3>

<p>PDF<a href="/assets/slides/14Feb_Model_Explantions_with_DP.pdf"> here</a>.</p>]]></content><author><name></name></author><category term="interpretability" /><category term="privacy" /><summary type="html"><![CDATA[Check out the paper Model Explanations with Differential Privacy - N. Patel, R. Shokri, and Y. Zick. 2022]]></summary></entry></feed>